{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizdoom import gymnasium_wrapper\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import skimage\n",
    "import keras\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericssonlin/miniconda3/envs/aiml/lib/python3.10/site-packages/vizdoom/gymnasium_wrapper/base_gymnasium_env.py:84: UserWarning: Detected screen format CRCGCB. Only RGB24 and GRAY8 are supported in the Gymnasium wrapper. Forcing RGB24.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "environment = gym.make(\"VizdoomBasic-v0\", render_mode = 'rgb_array')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.elements = []\n",
    "        self.buffer_limit = buffer_size\n",
    "        return None\n",
    "    def insert(self, element):\n",
    "        if len(self.elements) == self.buffer_limit:\n",
    "            self.elements.pop(random.randint(0, self.buffer_limit - 1))\n",
    "            self.elements.append(element)\n",
    "        else:\n",
    "            self.elements.append(element)\n",
    "        return None\n",
    "    def sample(self, count = 1):\n",
    "        result = []\n",
    "        for i in range(count):\n",
    "            result.append(self.elements[random.randint(0, len(self.elements) - 1)])\n",
    "        return result\n",
    "\n",
    "@keras.saving.register_keras_serializable()\n",
    "class DeepQ(keras.Layer):\n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernels,\n",
    "                 strides,\n",
    "                 dense_units,\n",
    "                 **kwargs):\n",
    "        super(DeepQ, self).__init__()\n",
    "        self.filters, self.kernels, self.strides, self.dense_units = filters, kernels, strides, dense_units\n",
    "        for i in range(len(filters)):\n",
    "            conv = keras.layers.Conv2D(filters = filters[i],\n",
    "                                       kernel_size = kernels[i],\n",
    "                                       strides = strides[i],\n",
    "                                       padding = 'same')\n",
    "            self._layers.append(conv)\n",
    "        self._layers.append(keras.layers.Flatten())\n",
    "        for i in range(len(dense_units)):\n",
    "            dense = keras.layers.Dense(units = dense_units[i])\n",
    "            self._layers.append(dense)\n",
    "        return None\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self._layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'filters': self.filters,\n",
    "            'kernels': self.kernels,\n",
    "            'strides': self.strides,\n",
    "            'dense_units': self.dense_units,\n",
    "        }\n",
    "        base_config = super(DeepQ, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        filters = config.pop('filters')\n",
    "        kernels = config.pop('kernels')\n",
    "        strides = config.pop('strides')\n",
    "        dense_units = config.pop('dense_units')\n",
    "        layer = cls(filters = filters,\n",
    "                    kernels = kernels,\n",
    "                    strides = strides,\n",
    "                    dense_units = dense_units,\n",
    "                    **config)\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(frame):\n",
    "    reduce_dims = frame[0]\n",
    "    gs = np.mean(reduce_dims, -1) / 255\n",
    "    cf = np.array(gs)[30:-10, 30:-30]\n",
    "    result = skimage.transform.resize(cf, [84, 84])\n",
    "    result = np.expand_dims(result, 0)\n",
    "    return result\n",
    "\n",
    "def stack_state(stack, state, is_new):\n",
    "    state = preprocess(state)\n",
    "    if is_new:\n",
    "        stack = deque([np.zeros((84, 84)) for i in range(4)], maxlen = 4)\n",
    "        for _ in range(4):\n",
    "            stack.append(state)\n",
    "    else:\n",
    "        stack.append(state)\n",
    "    tensor = np.stack(stack, -1)\n",
    "    return tensor, stack\n",
    "\n",
    "def sample_action(env: gym.Env, function: keras.Model, state, epsilon, verbosity):\n",
    "    if random.random() < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(function.predict(state, verbose = verbosity))\n",
    "\n",
    "def initialize_memory(env: gym.Env, buffer_size):\n",
    "    buffer = ReplayBuffer(buffer_size)\n",
    "    stack = deque([np.zeros((84, 84)) for i in range(4)], maxlen = 4)\n",
    "    state, info = env.reset()\n",
    "    state = state[\"screen\"]\n",
    "    state = np.expand_dims(state, 0)\n",
    "    state, stacked_states = stack_state(stack, state, True)\n",
    "    for i in range(1, buffer_size):\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = next_state[\"screen\"]\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        if terminated or truncated:\n",
    "            next_state = np.zeros((state.shape))\n",
    "            buffer.insert((state, action, reward, next_state, True))\n",
    "            state, info = env.reset()\n",
    "            state = state[\"screen\"]\n",
    "            state = np.expand_dims(state, 0)\n",
    "            state, stacked_states = stack_state(stack, state, True)\n",
    "        else:\n",
    "            next_state, stacked_states = stack_state(stacked_states, next_state, False)\n",
    "            buffer.insert((state, action, reward, next_state, False))\n",
    "            state = next_state\n",
    "    return buffer, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericssonlin/miniconda3/envs/aiml/lib/python3.10/site-packages/keras/src/layers/layer.py:357: UserWarning: `build()` was called on layer 'deep_q_5', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/Users/ericssonlin/miniconda3/envs/aiml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "input = keras.layers.Input(shape = (84, 84, 4))\n",
    "q = DeepQ(filters = [32, 64, 64],\n",
    "        kernels = [8, 4, 3],\n",
    "        strides = [4, 3, 1],\n",
    "        dense_units = [512, 4])\n",
    "output = q(input)\n",
    "function = keras.models.Model(input, output)\n",
    "function.compile(optimizer = keras.optimizers.Adam(0.0005), loss = 'mse')\n",
    "\n",
    "function.save('test.keras')\n",
    "\n",
    "r = keras.models.load_model('test.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, \n",
    "          episodes: int, \n",
    "          episode_length: int, \n",
    "          input_shape: tuple, \n",
    "          conv_filters: list, \n",
    "          conv_kernels: list, \n",
    "          conv_strides: list, \n",
    "          dense_units: list, \n",
    "          buffer_size: int, \n",
    "          learning_rate: float = 0.0005, \n",
    "          epsilon: float = 0.01, \n",
    "          epsilon_decay: float | None = None, \n",
    "          gamma: float = 0.999, \n",
    "          batch_size: int = 32, \n",
    "          reset_frequency: int = 16, \n",
    "          verbosity = 0, \n",
    "          id: int | str = 0) -> keras.Model:\n",
    "\n",
    "    assert(len(conv_filters) == len(conv_kernels) == len(conv_strides))\n",
    "    assert(len(dense_units) > 0)\n",
    "    assert(0 <= epsilon <= 1)\n",
    "\n",
    "    _w = math.floor(math.log10(episodes)) + 1\n",
    "    input = keras.layers.Input(shape = input_shape)\n",
    "    q = DeepQ(filters = conv_filters,\n",
    "            kernels = conv_kernels,\n",
    "            strides = conv_strides,\n",
    "            dense_units = dense_units)\n",
    "    output = q(input)\n",
    "    function = keras.models.Model(input, output)\n",
    "\n",
    "    target_input = keras.layers.Input(shape = input_shape)\n",
    "    target_q = DeepQ(filters = conv_filters,\n",
    "                    kernels = conv_kernels,\n",
    "                    strides = conv_strides,\n",
    "                    dense_units = dense_units)\n",
    "    target_output = target_q(target_input)\n",
    "    target_function = keras.models.Model(target_input, target_output)\n",
    "    \n",
    "    function.compile(optimizer = keras.optimizers.Adam(learning_rate), loss = 'mse')\n",
    "    target_function.compile(optimizer = keras.optimizers.Adam(learning_rate), loss = 'mse')\n",
    "    \n",
    "    episode_rewards = []\n",
    "    \n",
    "    best_reward = 0.0\n",
    "    \n",
    "    buffer, stack = initialize_memory(env, buffer_size)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        _st = time.time()\n",
    "        \n",
    "        episode_rewards.append(0.0)\n",
    "        state, info = env.reset()\n",
    "        state = state[\"screen\"]\n",
    "        state = np.expand_dims(state, 0)\n",
    "        state, stacked_states = stack_state(stack, state, True)\n",
    "        for step in range(episode_length):\n",
    "            if epsilon_decay is not None:\n",
    "                epsilon *= math.exp(-epsilon_decay)\n",
    "            action = sample_action(env, function, state, epsilon, verbosity)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = next_state[\"screen\"]\n",
    "            next_state = np.expand_dims(next_state, 0)\n",
    "            episode_rewards[-1] += reward\n",
    "            if terminated or truncated:\n",
    "                next_state = np.zeros((state.shape))\n",
    "                buffer.insert((state, action, reward, next_state, True))\n",
    "                state, info = env.reset()\n",
    "                state = state[\"screen\"]\n",
    "                state = np.expand_dims(state, 0)\n",
    "                state, stacked_states = stack_state(stack, state, True)\n",
    "                continue\n",
    "            else:\n",
    "                next_state, stacked_states = stack_state(stacked_states, next_state, False)\n",
    "                buffer.insert((state, action, reward, next_state, False))\n",
    "                state = next_state\n",
    "            \n",
    "            # Train\n",
    "            batch = buffer.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch], ndmin = 3)\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch]) \n",
    "            next_states = np.array([each[3] for each in batch], ndmin = 3)\n",
    "            dones = np.array([each[4] for each in batch])\n",
    "            \n",
    "            states = np.squeeze(states)\n",
    "            next_states = np.squeeze(next_states)\n",
    "            if batch_size == 1:\n",
    "                states = np.expand_dims(states, 0)\n",
    "                next_states = np.expand_dims(next_states, 0)\n",
    "            q_states = function.predict(states, verbose = verbosity)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                state, action, reward, next_state, done = states[i], actions[i], rewards[i], next_states[i], dones[i]\n",
    "                q_state = q_states[i]\n",
    "                q_target = reward\n",
    "                \n",
    "                state = np.expand_dims(state, 0)\n",
    "                next_state = np.expand_dims(next_state, 0)\n",
    "                \n",
    "                if not done:\n",
    "                    q_target += gamma * np.amax(target_function.predict(next_state, verbose = verbosity))\n",
    "                q_state[action] = q_target\n",
    "                function.fit(state, np.expand_dims(q_state, 0), verbose = verbosity)\n",
    "            \n",
    "            \n",
    "            _was_done_str = \"env terminated or truncated\" if terminated or truncated else \"env finished               \"\n",
    "            _et = time.time() - _st\n",
    "            _ec = episode + 1\n",
    "            _er = episodes - _ec\n",
    "            _eta = _et / _ec * _er\n",
    "            print(f\"Episode {(episode + 1): {_w}}/{episodes}\\t\"\n",
    "                  f\"[{'=' * math.floor(step * 25 / episode_length)}>\"\n",
    "                  f\"{'-' * (25 - math.floor(step * 25 / episode_length))}]\"\n",
    "                  f\"\\tFrame {step + 1} of {episode_length}, \"\n",
    "                  f\"{int(_et)}s elapsed); {_was_done_str}\", end = '\\r')\n",
    "\n",
    "            if (episode * episode_length + step) % reset_frequency == 0:\n",
    "                target_function.set_weights(function.get_weights())\n",
    "        \n",
    "        print(f\"Episode {episode + 1: {_w}}/{episodes}\\t[{'=' * 26}], \"\n",
    "              f\"{int(time.time() - _st)}s elapsed{' ' * 50}\")\n",
    "        if episode_rewards[-1] > best_reward:\n",
    "            print(f'Reward improved from {best_reward: .3f} to {episode_rewards[-1]: .3f}, '\n",
    "                  f'saving model to file \"policy_{id}.keras\"')\n",
    "            best_reward = episode_rewards[-1]\n",
    "            function.save(f'policy_{id}.keras')\n",
    "        else:\n",
    "            print(f'Reward of {episode_rewards[-1]: .3f} did not improve from {best_reward: .3f}')\n",
    "    \n",
    "    return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode  1/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -24.000 did not improve from  0.000\n",
      "Episode  2/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -120.000 did not improve from  0.000\n",
      "Episode  3/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -125.000 did not improve from  0.000\n",
      "Episode  4/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -24.000 did not improve from  0.000\n",
      "Episode  5/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -130.000 did not improve from  0.000\n",
      "Episode  6/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -120.000 did not improve from  0.000\n",
      "Episode  7/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -120.000 did not improve from  0.000\n",
      "Episode  8/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -115.000 did not improve from  0.000\n",
      "Episode  9/20\t[==========================], 42s elapsed                                                  \n",
      "Reward of -125.000 did not improve from  0.000\n",
      "Episode  10/20\t[==========================], 41s elapsed                                                  \n",
      "Reward improved from  0.000 to  82.000, saving model to file \"policy_thu25apr.keras\"\n",
      "Episode  11/20\t[==========================], 42s elapsed                                                  \n",
      "Reward of -125.000 did not improve from  82.000\n",
      "Episode  12/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -125.000 did not improve from  82.000\n",
      "Episode  13/20\t[==========================], 47s elapsed                                                  \n",
      "Reward of -125.000 did not improve from  82.000\n",
      "Episode  14/20\t[==========================], 43s elapsed                                                  \n",
      "Reward of -24.000 did not improve from  82.000\n",
      "Episode  15/20\t[==========================], 44s elapsed                                                  \n",
      "Reward of -130.000 did not improve from  82.000\n",
      "Episode  16/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -130.000 did not improve from  82.000\n",
      "Episode  17/20\t[==========================], 42s elapsed                                                  \n",
      "Reward of -120.000 did not improve from  82.000\n",
      "Episode  18/20\t[==========================], 41s elapsed                                                  \n",
      "Reward of -125.000 did not improve from  82.000\n",
      "Episode  19/20\t[==========================], 40s elapsed                                                  \n",
      "Reward of -24.000 did not improve from  82.000\n",
      "Episode  20/20\t[==========================], 40s elapsed                                                  \n",
      "Reward of  82.000 did not improve from  82.000\n"
     ]
    }
   ],
   "source": [
    "policy = train(env = environment,\n",
    "               episodes = 20,\n",
    "               episode_length = 100,\n",
    "               input_shape = (84, 84, 4),\n",
    "               conv_filters = [32, 64, 64],\n",
    "               conv_kernels = [8, 4, 3],\n",
    "               conv_strides = [4, 3, 1],\n",
    "               dense_units = [512, 4],\n",
    "               buffer_size = 1000,\n",
    "               learning_rate = 0.005,\n",
    "               epsilon = 0.1,\n",
    "               epsilon_decay = 0.003,\n",
    "               gamma = 0.995,\n",
    "               batch_size = 4,\n",
    "               reset_frequency = 50,\n",
    "               verbosity = 0,\n",
    "               id = \"thu25apr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericssonlin/miniconda3/envs/aiml/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/ericssonlin/code/Python/AIML/RL folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/ericssonlin/code/Python/AIML/RL/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/ericssonlin/code/Python/AIML/RL/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/ericssonlin/code/Python/AIML/RL/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/ericssonlin/code/Python/AIML/RL/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/ericssonlin/code/Python/AIML/RL/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/ericssonlin/code/Python/AIML/RL/rl-video-episode-1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-500.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_policy(env: gym.Env, policy, episode_length, video_path, buffer_size, episode_trigger, fps, verbosity):\n",
    "    r_env = gym.wrappers.RecordVideo(env, video_path, episode_trigger = episode_trigger)\n",
    "    r_env.metadata['video.frames_per_second'] = fps\n",
    "\n",
    "    rewards = 0.0\n",
    "    \n",
    "    buffer, stack = initialize_memory(r_env, buffer_size)\n",
    "    state, info = r_env.reset()\n",
    "    state = state[\"screen\"]\n",
    "    state = np.expand_dims(state, 0)\n",
    "    state, stacked_states = stack_state(stack, state, True)\n",
    "    for step in range(episode_length):\n",
    "        action = np.argmax(policy.predict(state, verbose = verbosity))\n",
    "        next_state, reward, terminated, truncated, info = r_env.step(action)\n",
    "        next_state = next_state[\"screen\"]\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "        rewards += reward\n",
    "        if terminated or truncated:\n",
    "            next_state = np.zeros((state.shape))\n",
    "            buffer.insert((state, action, reward, next_state, True))\n",
    "            state, info = env.reset()\n",
    "            state = state[\"screen\"]\n",
    "            state = np.expand_dims(state, 0)\n",
    "            state, stacked_states = stack_state(stack, state, True)\n",
    "            continue\n",
    "        else:\n",
    "            next_state, stacked_states = stack_state(stacked_states, next_state, False)\n",
    "            buffer.insert((state, action, reward, next_state, False))\n",
    "            state = next_state\n",
    "            \n",
    "    r_env.close_video_recorder()\n",
    "    r_env.close()\n",
    "    return rewards\n",
    "\n",
    "evaluate_policy(env = environment,\n",
    "                policy = policy,\n",
    "                episode_length = 500,\n",
    "                video_path = './',\n",
    "                buffer_size = 100,\n",
    "                episode_trigger = lambda x: True,\n",
    "                fps = 8,\n",
    "                verbosity = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
